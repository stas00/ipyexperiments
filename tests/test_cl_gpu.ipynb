{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CellLogger subsystem\n",
    "\n",
    "This test tests the CellLogger subsystem, accessible via `exp.cl` once one of the `IPyExperiments` subclasses has been initiated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test specifics\n",
    "\n",
    "Since we need to validate the the output, we have to capture it first. The way jupyter is setup, is that in once cell you set up a capture with `%%capture` magick and then in the next cell you can analyze it. That's why each test group has two cells, the first one doing the action to be tested and the following one doing the validatations.\n",
    "\n",
    "Moreover, the output of this test becomes confusing because the capture mechanism somehow messes things up which leads to re-running the `post_run_cell` callback of the CellLogger subsystem again - as a result you get a bogus output with 0's regardless of the code being run. It doesn't interfere with the testing, but it does interfere with things like `.data` which gets reset because of that, showing invalid information - therefore we can only test `.data` w/o capturing the cell's output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import isclose\n",
    "from ipyexperiments import *\n",
    "import re, numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consume_cpu_ram(n): return np.ones((n, n))\n",
    "def consume_gpu_ram(n): return torch.ones((n, n)).cuda()\n",
    "def consume_cpu_ram_128mb():  return consume_cpu_ram(2**12)\n",
    "def consume_gpu_ram_1024mb(): return consume_gpu_ram(2**14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------- #\n",
    "# the following functions work with the captured output\n",
    "# output is captured by `%%capture output` from a cell before\n",
    "\n",
    "# convert .data outputs to the same dimensions (MBs) as reports\n",
    "def b2mbi(x): print(x); return int(x/2**20)\n",
    "\n",
    "# sample:\n",
    "# RAM: Consumed Peaked  Used Total | Exec time 0.046s\n",
    "def check_report_strings(output):\n",
    "    # basic checks\n",
    "    to_match = [r'Consumed', 'Peaked']\n",
    "    for s in to_match: assert re.search(s, output), f\"expecting string: {s}\"\n",
    "\n",
    "# sample:        \n",
    "# CPU:      123    321     2159 MB\n",
    "# GPU:      356    789     2160 MB\n",
    "def get_sizes(output, type):\n",
    "    match = re.findall(type + r': +([\\d\\.]+) +([\\d\\.]+) +([\\d\\.]+) MB', output)\n",
    "    (consumed, peaked, total) = map(float, match[0])\n",
    "    return consumed, peaked, total\n",
    "\n",
    "def get_sizes_cpu(output): return get_sizes(output, \"CPU\")\n",
    "def get_sizes_gpu(output): return get_sizes(output, \"GPU\")\n",
    "\n",
    "# compare reported numbers against expected\n",
    "def check_match(consumed_reported, peaked_reported, \n",
    "                consumed_expected, peaked_expected, abs_tol=0):\n",
    "    assert isclose(consumed_reported, consumed_expected, abs_tol=abs_tol), f\"Consumed RAM reported: {consumed_reported} == real: {consumed_expected}\"\n",
    "    assert isclose(peaked_reported,   peaked_expected,   abs_tol=abs_tol), f\"Peaked RAM reported: {peaked_reported} == real: {peaked_expected}\"\n",
    "\n",
    "# these functions extract the reported data from the output\n",
    "def check_report_cpu(output, consumed_expected, peaked_expected, abs_tol=0):\n",
    "    consumed_reported, peaked_reported, total_reported = get_sizes_cpu(output)\n",
    "    check_match(consumed_reported, peaked_reported, \n",
    "                consumed_expected, peaked_expected, abs_tol)\n",
    "\n",
    "def check_report_gpu(output, consumed_expected, peaked_expected, abs_tol=0):\n",
    "    consumed_reported, peaked_reported, total_reported = get_sizes_gpu(output)\n",
    "    check_match(consumed_reported, peaked_reported, \n",
    "                consumed_expected, peaked_expected, abs_tol)\n",
    "def print_output(output):\n",
    "    print(\"Captured output:\\n\" + \"-\"*50 + \"\\n\" + output + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, GeForce GTX 1070 Ti (7.9 GB RAM)\n",
      "\n",
      "RAM: Consumed Peaked  Used Total | Exec time 0.000s\n",
      "CPU:        0      0     2159 MB |\n",
      "GPU:        0      0      996 MB |\n"
     ]
    }
   ],
   "source": [
    "#if 'il' in locals(): il.stop() # helps debug\n",
    "exp1 = IPyExperimentsPytorch(enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM: Consumed Peaked  Used Total | Exec time 0.367s\n",
      "CPU:        0      0     2287 MB |\n",
      "GPU:        0      0     2020 MB |\n"
     ]
    }
   ],
   "source": [
    "%%capture output\n",
    "cpu1 = consume_cpu_ram_128mb()\n",
    "gpu1 = consume_gpu_ram_1024mb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_report'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured output:\n",
      "--------------------------------------------------\n",
      "RAM: Consumed Peaked  Used Total | Exec time 0.357s\n",
      "CPU:      128      0     2287 MB |\n",
      "GPU:     1024      0     2020 MB |\n",
      "--------------------------------------------------\n",
      "\n",
      "RAM: Consumed Peaked  Used Total | Exec time 0.010s\n",
      "CPU:        0      0     2159 MB |\n",
      "GPU:    -1024   1024      996 MB |\n"
     ]
    }
   ],
   "source": [
    "\"\"\"test_report\"\"\"\n",
    "output = str(output)\n",
    "print_output(output)\n",
    "\n",
    "check_report_strings(output)\n",
    "check_report_cpu(output, consumed_expected= 128, peaked_expected=0, abs_tol=2)\n",
    "check_report_gpu(output, consumed_expected=1024, peaked_expected=0, abs_tol=0)\n",
    "\n",
    "# cleanup\n",
    "del cpu1, gpu1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consume/release leading to positive peak numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM: Consumed Peaked  Used Total | Exec time 0.730s\n",
      "CPU:        0      0     2287 MB |\n",
      "GPU:        0   1024     2020 MB |\n"
     ]
    }
   ],
   "source": [
    "%%capture output\n",
    "# test peak measurement\n",
    "# here we consume 256MB of RAM and release 128MB \n",
    "# testing: Consumed 128, Peaked 128\n",
    "cpu1 = consume_cpu_ram_128mb()\n",
    "cpu2 = consume_cpu_ram_128mb()\n",
    "del cpu1\n",
    "\n",
    "# here we consume 2048MB of RAM and release 1024MB\n",
    "# testing: Consumed 1024, Peaked 1024\n",
    "gpu1 = consume_gpu_ram_1024mb()\n",
    "gpu2 = consume_gpu_ram_1024mb()\n",
    "del gpu1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_peak_memory_usage'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured output:\n",
      "--------------------------------------------------\n",
      "RAM: Consumed Peaked  Used Total | Exec time 0.719s\n",
      "CPU:      128    128     2287 MB |\n",
      "GPU:     1024   1024     2020 MB |\n",
      "--------------------------------------------------\n",
      "\n",
      "RAM: Consumed Peaked  Used Total | Exec time 0.005s\n",
      "CPU:        0      0     2159 MB |\n",
      "GPU:    -1024   1024      996 MB |\n"
     ]
    }
   ],
   "source": [
    "\"\"\"test_peak_memory_usage\"\"\"\n",
    "output = str(output)\n",
    "print_output(output)\n",
    "\n",
    "check_report_cpu(output, consumed_expected= 128, peaked_expected= 128, abs_tol=2)\n",
    "check_report_gpu(output, consumed_expected=1024, peaked_expected=1024, abs_tol=2)\n",
    "\n",
    "# cleanup\n",
    "del cpu2, gpu2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .data accessor validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM: Consumed Peaked  Used Total | Exec time 0.765s\n",
      "CPU:      128    128     2287 MB |\n",
      "GPU:     1024   1024     2020 MB |\n"
     ]
    }
   ],
   "source": [
    "# no capture! breaks .data since it re-runs the post_run_cell, again, resetting .data\n",
    "# here we consume 256MB of RAM and release 128MB - so that we can test peak measurement\n",
    "# testing: Consumed 128, Peaked 128\n",
    "cpu1 = consume_cpu_ram_128mb()\n",
    "cpu2 = consume_cpu_ram_128mb()\n",
    "del cpu1\n",
    "#assert 5==6, \"really?\"\n",
    "# here we consume 256MB of RAM and release 128MB - so that we can test peak measurement\n",
    "# testing: \n",
    "gpu1 = consume_gpu_ram_1024mb()\n",
    "gpu2 = consume_gpu_ram_1024mb()\n",
    "## Consume/Release Positive Peak\n",
    "del gpu1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_data_accessor'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM: Consumed Peaked  Used Total | Exec time 0.005s\n",
      "CPU:        0      0     2159 MB |\n",
      "GPU:    -1024   1024      996 MB |\n"
     ]
    }
   ],
   "source": [
    "\"\"\"test_data_accessor\"\"\"\n",
    "cpu_mem, gpu_mem, time_data = exp1.cl.data\n",
    "\n",
    "check_match(consumed_reported=cpu_mem.used_delta, peaked_reported=cpu_mem.peaked_delta, \n",
    "            consumed_expected=128,                peaked_expected=128,  abs_tol=1)\n",
    "check_match(consumed_reported=gpu_mem.used_delta, peaked_reported=gpu_mem.peaked_delta, \n",
    "            consumed_expected=1024,               peaked_expected=1024, abs_tol=1)\n",
    "\n",
    "# cleanup\n",
    "del cpu2, gpu2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_stop'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM: Consumed Peaked  Used Total | Exec time 0.028s\n",
      "CPU:        0      0     2159 MB |\n",
      "GPU:        0      0      996 MB |\n"
     ]
    }
   ],
   "source": [
    "\"\"\"test_stop\"\"\"\n",
    "exp1.cl.stop()\n",
    "#check that no output appears after this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture output\n",
    "cpu1 = consume_cpu_ram_128mb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_report'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured output:\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"test_report\"\"\"\n",
    "output = str(output)\n",
    "print_output(output)\n",
    "assert output == \"\", \"there should be no output as logger has been stopped\"\n",
    "\n",
    "# cleanup\n",
    "del cpu1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_notebook()\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript # prevent committing an unsaved notebook\n",
    "IPython.notebook.save_notebook()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "285px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "323px",
    "left": "956px",
    "right": "20px",
    "top": "152px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
